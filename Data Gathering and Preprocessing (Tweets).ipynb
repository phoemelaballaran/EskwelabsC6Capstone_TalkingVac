{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Twitter Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv('datasets/Metro Manila Barangays.csv',index_col=0)\n",
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(city):\n",
    "    \n",
    "    location = locations[locations['CityName'] == city] \n",
    "    location = location.reset_index()\n",
    "\n",
    "    data = []\n",
    "    keywords = ['Vaccine OR Vaccination OR Vax OR CovidVaccine OR CovidVaccination OR CovidVax OR CovidPHVaccine OR CoronavirusVaccine OR CoronaVaccine OR CoronavirusVaccination OR CoronaVax OR VaccinePH OR COVID19Vaccine OR GetVaccinated OR Sinovac OR Astrazeneca OR AZ OR Coronovac OR Novovax OR SputnikV OR Pfizer OR Moderna']\n",
    "\n",
    "    for i in range(0,len(location)):\n",
    "\n",
    "        start_date = '2020-12-12'\n",
    "        end_date = '2021-03-12'\n",
    "        center = location['Center'][i].replace('(','').replace(')','').rpartition(',')\n",
    "        radius = location['Radius'][i]\n",
    "\n",
    "        twscrape = sntwitter.TwitterSearchScraper(f\"{keywords} geocode:{center[2].strip()},{center[0]},{radius}km since:{start_date} until:{end_date}\").get_items()\n",
    "\n",
    "        for x,tweet in enumerate(twscrape):\n",
    "            data.append([tweet.id, tweet.date, tweet.content,tweet.user.id])\n",
    "\n",
    "        print(f'Done with Barangay {i}')\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tweets from Metro Manila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Manila'\n",
    "manila = get_tweets(city)\n",
    "df_manila = pd.DataFrame(manila, columns=['id','date','tweet','user_id'])\n",
    "df_manila['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Kalookan City'\n",
    "kalookan = get_tweets(city)\n",
    "df_kalookan = pd.DataFrame(kalookan, columns=['id','date','tweet','user_id'])\n",
    "df_kalookan['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Las Piñas'\n",
    "laspinas = get_tweets(city)\n",
    "df_laspinas = pd.DataFrame(laspinas, columns=['id','date','tweet','user_id'])\n",
    "df_laspinas['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Makati City'\n",
    "makati = get_tweets(city)\n",
    "df_makati = pd.DataFrame(makati, columns=['id','date','tweet','user_id'])\n",
    "df_makati['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Malabon'\n",
    "malabon = get_tweets(city)\n",
    "df_malabon = pd.DataFrame(malabon, columns=['id','date','tweet','user_id'])\n",
    "df_malabon['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Marikina'\n",
    "marikina = get_tweets(city)\n",
    "df_marikina = pd.DataFrame(marikina, columns=['id','date','tweet','user_id'])\n",
    "df_marikina['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Muntinlupa'\n",
    "muntinlupa = get_tweets(city)\n",
    "df_muntinlupa = pd.DataFrame(muntinlupa, columns=['id','date','tweet','user_id'])\n",
    "df_muntinlupa['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Navotas'\n",
    "navotas = get_tweets(city)\n",
    "df_navotas = pd.DataFrame(navotas, columns=['id','date','tweet','user_id'])\n",
    "df_navotas['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Parañaque'\n",
    "paranaque = get_tweets(city)\n",
    "df_paranaque = pd.DataFrame(paranaque, columns=['id','date','tweet','user_id'])\n",
    "df_paranaque['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Pasay City'\n",
    "pasay = get_tweets(city)\n",
    "df_pasay = pd.DataFrame(pasay, columns=['id','date','tweet','user_id'])\n",
    "df_pasay['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Pasig City'\n",
    "pasig = get_tweets(city)\n",
    "df_pasig = pd.DataFrame(pasig, columns=['id','date','tweet','user_id'])\n",
    "df_pasig['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Pateros'\n",
    "pateros = get_tweets(city)\n",
    "df_pateros = pd.DataFrame(pateros, columns=['id','date','tweet','user_id'])\n",
    "df_pateros['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Quezon City'\n",
    "quezon = get_tweets(city)\n",
    "df_quezon = pd.DataFrame(quezon, columns=['id','date','tweet','user_id'])\n",
    "df_quezon['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'San Juan'\n",
    "sanjuan = get_tweets(city)\n",
    "df_sanjuan = pd.DataFrame(sanjuan, columns=['id','date','tweet','user_id'])\n",
    "df_sanjuan['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Taguig'\n",
    "taguig = get_tweets(city)\n",
    "df_taguig = pd.DataFrame(taguig, columns=['id','date','tweet','user_id'])\n",
    "df_taguig['city'] = city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Valenzuela'\n",
    "valenzuela = get_tweets(city)\n",
    "df_valenzuela = pd.DataFrame(valenzuela, columns=['id','date','tweet','user_id'])\n",
    "df_valenzuela['city'] = city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [df_manila, df_kalookan, df_laspinas, df_makati, df_malabon, df_marikina, df_muntinlupa, df_navotas, df_paranaque, df_pasay, df_pasig, df_pateros, df_quezon, df_sanjuan, df_taguig, df_valenzuela]\n",
    "df_combined = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[df_combined.duplicated(subset='id', keep = 'first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaning Tweet Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_combined.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unrelated tweets: \"Az önce bir fotoğraf paylaştı\" and \" \"Az önce bir video paylaştı\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3450, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean[~df_clean['tweet'].str.contains(\"Az önce bir fotoğraf paylaştı\")]\n",
    "df_clean = df_clean[~df_clean['tweet'].str.contains(\"Az önce bir video paylaştı\")]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove tweets that use 'az' in replacement of 'as' or as a name (not related to covid vaccine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3406, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az = [1337757676822884358, 1338360256334680064, 1343889294667071488, 1344678416499372032, 1344691248083292165,\n",
    "      1345186676092538880, 1345622379494457345, 1346484682033045509, 1347522905014370310, 1349619038612148224,\n",
    "      1349684110038454272, 1349717339088973828, 1355096253999136774, 1357127077166632960, 1357725726150041600, \n",
    "      1357766343831953409, 1359406021249179648, 1360927700173475842, 1363364189910093824, 1364057664100003843,\n",
    "      1364453286070292480, 1364453598730444802, 1364922835991359494, 1365530374705569792, 1366960011650367494,\n",
    "      1367145684781072388, 1368131951186509830, 1368819920692154372, 1368880519878901761, 1368884913907240963, \n",
    "      1343211873022271488, 1343339915736563712, 1343407965445865472, 1344966999504375808, 1348325075347415043,\n",
    "      1353640720846921728, 1353732382101147649, 1353925240930197511, 1353985692221620225, 1354412411239849998,\n",
    "      1354623401889984512, 1359936514130538496, 1364404897337122816, 1364453961185456130, 1368852188647190528]\n",
    "\n",
    "df_clean = df_clean[~df_clean.id.isin(az)]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove tweets that mentioned a different vaccine (flu vaccine, vaccine for babies or dogs), or 'vax' pertains to something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3350, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vax = [1355064005857996803, 1356911945404702721, 1357672278780092420, 1359400457806901248, 1359425088538333185,\n",
    "       1361227642159128577, 1361871108706734083, 1362040840768524288, 1362041092842000385, 1362183620597743620, \n",
    "       1362262110571388930, 1362323704286355461, 1362955263683088384, 1364766305409142786, 1364798054927228929, \n",
    "       1365646924343205889, 1366654379286142976, 1367129681883897857, 1369500504787623943, 1369883255818776576, \n",
    "       1347864324031746053, 1339947339964841985, 1339985068001951744, 1340058614573092865, 1342138309212958722,\n",
    "       1342419755886514176, 1343087831401385987, 1343365504640057346, 1347012670386442241, 1347890469594750977,\n",
    "       1348117550236803072, 1348864554080485376, 1349989521337643008, 1350682076153044994, 1352877552314982400,\n",
    "       1353166959974567936, 1353692297787961356, 1354869996682309635, 1355428220561203200, 1356091650020270083,\n",
    "       1357235263026593793, 1358060002968145927, 1358951981507743744, 1359141872740962306, 1359732067345514497,\n",
    "       1362650420292718594, 1368863923223560194, 1358707139452231687, 1349011192849854473, 1351436243091898368,\n",
    "       1343405743764951042, 1343399988034818049, 1363827816349933568, 1367340673314082818, 1353669189743984640,\n",
    "       1353912925132480512]\n",
    "\n",
    "df_clean = df_clean[~df_clean.id.isin(vax)]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove tweets that are using a different dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3343, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dia = [1366027449583079424, 1365917905330733056, 1359292644128559104, 1360050936111009795, 1361594369401364483, 1368882692805160960, 1364914496242782211]\n",
    "df_clean = df_clean[~df_clean.id.isin(dia)]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove tweets with chinese characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3340, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi = [1348293590535995392, 1350376481789120514, 1343056322711830530]\n",
    "df_clean = df_clean[~df_clean.id.isin(chi)]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Totally unrelated but has vaccine name (like *Moderna*) on the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3339, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unre = [1341602022110326784]\n",
    "df_clean = df_clean[~df_clean.id.isin(unre)]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sketchy tweet from a sketchy account. Example tweet: *Im correct again about the vaccine yehey mybdayoct.7,1981color blue I hve7folwing&amp;0folwersexctpckup jan.31,21 6am-8am 2ndst.prpetualextensn @taylorswift13 @GiGiHadid @MileyCyrus @selenagomez @karliekloss  @FBI woman  @CIA womanI’mtheonewhopost this @jsb10fbi7blue81 https://t.co/TzitAYWije*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3333, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ske = [1342191289429725184, 1341295719517319168, 1341837990293721088,\n",
    "       1351331399538622464, 1351333728597905409, 1358946633174503424]\n",
    "df_clean = df_clean[~df_clean.id.isin(ske)]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('unique.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cleaning Tweet Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_format = df_clean.copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].str.replace('&amp;', 'and')\n",
    "df['tweet'] = df['tweet'].str.replace(' http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '')\n",
    "df['tweet'] = df['tweet'].str.replace('$gt;', '>')\n",
    "df['tweet'] = df['tweet'].str.replace('$lt;', '<')\n",
    "df['tweet'] = df['tweet'].str.replace('u ', 'you ')\n",
    "df['tweet'] = df['tweet'].str.replace('\\u200d', '')\n",
    "df['tweet'] = df['tweet'].str.replace('\\n', ' ')\n",
    "df['tweet'] = df['tweet'].str.lower()\n",
    "df['tweet'] = df['tweet'].str.replace('(?:\\@|http?\\://|https?\\://|www)\\S+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get Number of Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt = df_format.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt = df_rt.groupby('id')[['tweet']].count()\n",
    "df_rt = df_rt.reset_index().rename(columns={'tweet': 'retweets'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct = df_format.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "df_distinct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge df_rt and df_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.merge(df_distinct, df_rt, how=\"left\", on= 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Translate Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translate = df_merge.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translate = df_translate.drop('index', 1)\n",
    "df_fil_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from deep_translator.constants import BASE_URLS, GOOGLE_LANGUAGES_TO_CODES\n",
    "from deep_translator.exceptions import TooManyRequests, LanguageNotSupportedException, TranslationNotFound, NotValidPayload, RequestError\n",
    "from deep_translator.parent import BaseTranslator\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "import warnings\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleTranslator(BaseTranslator):\n",
    "    \"\"\"\n",
    "    class that wraps functions, which use google translate under the hood to translate text(s)\n",
    "    \"\"\"\n",
    "    _languages = GOOGLE_LANGUAGES_TO_CODES\n",
    "    supported_languages = list(_languages.keys())\n",
    "\n",
    "    def __init__(self, source=\"auto\", target=\"en\"):\n",
    "        \"\"\"\n",
    "        @param source: source language to translate from\n",
    "        @param target: target language to translate to\n",
    "        \"\"\"\n",
    "        self.__base_url = BASE_URLS.get(\"GOOGLE_TRANSLATE\")\n",
    "\n",
    "        if self.is_language_supported(source, target):\n",
    "            self._source, self._target = self._map_language_to_code(source.lower(), target.lower())\n",
    "\n",
    "        super(GoogleTranslator, self).__init__(base_url=self.__base_url,\n",
    "                                               source=self._source,\n",
    "                                               target=self._target,\n",
    "                                               element_tag='div',\n",
    "                                               element_query={\"class\": \"t0\"},\n",
    "                                               payload_key='q',  # key of text in the url\n",
    "                                               hl=self._target,\n",
    "                                               sl=self._source)\n",
    "\n",
    "        self._alt_element_query = {\"class\": \"result-container\"}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_supported_languages(as_dict=False):\n",
    "        \"\"\"\n",
    "        return the supported languages by the google translator\n",
    "        @param as_dict: if True, the languages will be returned as a dictionary mapping languages to their abbreviations\n",
    "        @return: list or dict\n",
    "        \"\"\"\n",
    "        return GoogleTranslator.supported_languages if not as_dict else GoogleTranslator._languages\n",
    "\n",
    "    def _map_language_to_code(self, *languages):\n",
    "        \"\"\"\n",
    "        map language to its corresponding code (abbreviation) if the language was passed by its full name by the user\n",
    "        @param languages: list of languages\n",
    "        @return: mapped value of the language or raise an exception if the language is not supported\n",
    "        \"\"\"\n",
    "        for language in languages:\n",
    "            if language in self._languages.values() or language == 'auto':\n",
    "                yield language\n",
    "            elif language in self._languages.keys():\n",
    "                yield self._languages[language]\n",
    "            else:\n",
    "                raise LanguageNotSupportedException(language)\n",
    "\n",
    "    def is_language_supported(self, *languages):\n",
    "        \"\"\"\n",
    "        check if the language is supported by the translator\n",
    "        @param languages: list of languages\n",
    "        @return: bool or raise an Exception\n",
    "        \"\"\"\n",
    "        for lang in languages:\n",
    "            if lang != 'auto' and lang not in self._languages.keys():\n",
    "                if lang != 'auto' and lang not in self._languages.values():\n",
    "                    raise LanguageNotSupportedException(lang)\n",
    "        return True\n",
    "\n",
    "    def translate(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        function that uses google translate to translate a text\n",
    "        @param text: desired text to translate\n",
    "        @return: str: translated text\n",
    "        \"\"\"\n",
    "\n",
    "        if self._validate_payload(text):\n",
    "            text = text.strip()\n",
    "\n",
    "            if self.payload_key:\n",
    "                self._url_params[self.payload_key] = text\n",
    "\n",
    "            response = requests.get(self.__base_url,\n",
    "                                    params=self._url_params, headers ={'User-agent': 'your bot 0.1'})\n",
    "\n",
    "            if response.status_code == 429:\n",
    "                raise TooManyRequests()\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                raise RequestError()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            element = soup.find(self._element_tag, self._element_query)\n",
    "\n",
    "            if not element:\n",
    "                element = soup.find(self._element_tag, self._alt_element_query)\n",
    "                if not element:\n",
    "                    raise TranslationNotFound(text)\n",
    "            if element.get_text(strip=True) == text.strip():\n",
    "                to_translate_alpha = ''.join(ch for ch in text.strip() if ch.isalnum())\n",
    "                translated_alpha = ''.join(ch for ch in element.get_text(strip=True) if ch.isalnum())\n",
    "                if to_translate_alpha and translated_alpha and to_translate_alpha == translated_alpha:\n",
    "                    self._url_params[\"tl\"] = self._target\n",
    "                    if \"hl\" not in self._url_params:\n",
    "                        return text.strip()\n",
    "                    del self._url_params[\"hl\"]\n",
    "                    return self.translate(text)\n",
    "\n",
    "            else:\n",
    "                return element.get_text(strip=True)\n",
    "\n",
    "    def translate_file(self, path, **kwargs):\n",
    "        \"\"\"\n",
    "        translate directly from file\n",
    "        @param path: path to the target file\n",
    "        @type path: str\n",
    "        @param kwargs: additional args\n",
    "        @return: str\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                text = f.read().strip()\n",
    "            return self.translate(text)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def translate_sentences(self, sentences=None, **kwargs):\n",
    "        \"\"\"\n",
    "        translate many sentences together. This makes sense if you have sentences with different languages\n",
    "        and you want to translate all to unified language. This is handy because it detects\n",
    "        automatically the language of each sentence and then translate it.\n",
    "        @param sentences: list of sentences to translate\n",
    "        @return: list of all translated sentences\n",
    "        \"\"\"\n",
    "        warnings.warn(\"deprecated. Use the translate_batch function instead\", DeprecationWarning, stacklevel=2)\n",
    "        logging.warning(\"deprecated. Use the translate_batch function instead\")\n",
    "        if not sentences:\n",
    "            raise NotValidPayload(sentences)\n",
    "\n",
    "        translated_sentences = []\n",
    "        try:\n",
    "            for sentence in sentences:\n",
    "                translated = self.translate(text=sentence)\n",
    "                translated_sentences.append(translated)\n",
    "\n",
    "            return translated_sentences\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def translate_batch(self, batch=None):\n",
    "        \"\"\"\n",
    "        translate a list of texts\n",
    "        @param batch: list of texts you want to translate\n",
    "        @return: list of translations\n",
    "        \"\"\"\n",
    "        if not batch:\n",
    "            raise Exception(\"Enter your text list that you want to translate\")\n",
    "\n",
    "        print(\"Please wait.. This may take a while because deep_translator sleeps \"\n",
    "              \"for a few seconds after each request in order to not spam the google server.\")\n",
    "        arr = []\n",
    "        for i, text in enumerate(batch):\n",
    "\n",
    "            translated = self.translate(text)\n",
    "            arr.append(translated)\n",
    "            print(\"sentence number \", i+1, \" has been translated successfully\")\n",
    "            sleep(random.randint(2,15))\n",
    "\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Translation(sentence):\n",
    "    translated = GoogleTranslator(source='tl', target='en').translate_batch(sentence)\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_translated = []\n",
    "tweet = df_translate['tweet'].tolist()\n",
    "tweet_translated.append(get_Translation(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert tweet_translated list to df_translated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translated = pd.DataFrame(np.array(tweet_translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to join df_translated with df_translate, we must manually create an index column which will serve as an ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translated['index'] = range(0, len(df_translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join df_translated with df_translate by using the actual index from df_translate and the manually created 'index' column from df_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df_translate,df_translated, left_index=True, right_on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('datasets/translated_tweets.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are tweets that Google Translate API wasn't able to translate so manually check and translate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
